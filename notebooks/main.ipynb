{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System modules\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import requests\n",
    "from hashlib import md5\n",
    "import json\n",
    "\n",
    "# ML modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Viz modules\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Append source directory to system path\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Helper functions\n",
    "import data.helpers as data_helpers\n",
    "import visualization.helpers as viz_helpers\n",
    "\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "YELP_CLIENT_ID = os.getenv(\"YELP_CLIENT_ID\")\n",
    "YELP_API_KEY = os.getenv(\"YELP_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_business_search_by_location(\n",
    "    location: str = \"Paris\", count: int = 200\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get Yelp data from API.\n",
    "\n",
    "    Params:\n",
    "        location: str\n",
    "        count: int\n",
    "\n",
    "    Returns:\n",
    "        json\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {YELP_API_KEY}\",\n",
    "    }\n",
    "    limit = 50\n",
    "\n",
    "    businesses = pd.DataFrame()\n",
    "    reviews = pd.DataFrame()\n",
    "    photos = pd.DataFrame()\n",
    "    for offset in range(0, count, limit):\n",
    "        businesses_request = requests.get(\n",
    "            \"https://api.yelp.com/v3/businesses/search\",\n",
    "            headers=headers,\n",
    "            params={\n",
    "                \"location\": location,\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if businesses_request.status_code == 200:\n",
    "            businesses_data = businesses_request.json()\n",
    "            businesses = businesses.append(\n",
    "                pd.DataFrame(businesses_data[\"businesses\"]), ignore_index=True\n",
    "            )\n",
    "\n",
    "            for business in businesses_data[\"businesses\"]:\n",
    "                business_detail_request = requests.get(\n",
    "                    f\"https://api.yelp.com/v3/businesses/{business['id']}\",\n",
    "                    headers=headers,\n",
    "                )\n",
    "                if business_detail_request.status_code == 200:\n",
    "                    business_detail_data = business_detail_request.json()\n",
    "                    photos = photos.append(\n",
    "                        pd.DataFrame(business_detail_data[\"photos\"]),\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        f\"Yelp API request failed with status code \\\n",
    "                             { business_detail_request.status_code }. \\\n",
    "                                 Response text: { business_detail_request.text }\"\n",
    "                    )\n",
    "\n",
    "            for business in businesses_data[\"businesses\"]:\n",
    "                business_reviews_request = requests.get(\n",
    "                    f\"https://api.yelp.com/v3/businesses/{business['id']}/reviews\",\n",
    "                    headers=headers,\n",
    "                )\n",
    "                if business_reviews_request.status_code == 200:\n",
    "                    business_reviews_data = business_reviews_request.json()\n",
    "                    reviews = reviews.append(\n",
    "                        pd.DataFrame(business_reviews_data[\"reviews\"]),\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        f\"Yelp API request failed with status code { business_reviews_request.status_code }. Response text: { business_reviews_request.text }\"\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Yelp API request failed with status code { businesses_request.status_code }. Response text: { businesses_request.text }\"\n",
    "            )\n",
    "\n",
    "    return businesses, reviews, photos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_data(\n",
    "    locations: list[str] = [\n",
    "        \"Paris\",\n",
    "        \"New York City\",\n",
    "        \"Tokyo\",\n",
    "        \"Rio de Janeiro\",\n",
    "        \"Sydney\",\n",
    "    ],\n",
    "    category: str = \"restaurants\",\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get Yelp data from API.\n",
    "\n",
    "    - iterate over the locations\n",
    "        - build a GraphQL query to get the data\n",
    "        - send the query to the Yelp API\n",
    "        - parse the response\n",
    "        - append to the dataframe\n",
    "    - return the dataframes\n",
    "\n",
    "    Params:\n",
    "        locations: str[] (default: [\"Paris\"]) - List of Yelp locations to search\n",
    "        category: str (default: \"restaurants\") - Yelp category (see https://www.yelp.com/developers/documentation/v3/all_category_list)\n",
    "\n",
    "    Returns:\n",
    "        businesses: pd.DataFrame - businesses data from Yelp API request\n",
    "        reviews: pd.DataFrame - reviews data from Yelp API request\n",
    "        photos: pd.DataFrame - photos data from Yelp API request\n",
    "    \"\"\"\n",
    "    # businesses data (see https://www.yelp.com/developers/graphql/objects/business)\n",
    "    businesses = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"business_alias\",  # Unique Yelp alias of this business.\n",
    "            \"business_review_count\",  # Total number of reviews for this business.\n",
    "            \"business_rating\",  # Rating of the business, which is an average of the ratings of all reviews.\n",
    "            \"business_price\",  # Price range of the business, from \"$\" to \"$$$$\" (inclusive).\n",
    "            \"business_city\",  # City of this business.\n",
    "            \"business_state\",  # ISO 3166-2 (with a few exceptions) state code of this business (see https://www.yelp.com/developers/documentation/v3/state_codes).\n",
    "            \"business_postal_code\",  # Postal code of this business (see https://en.wikipedia.org/wiki/Postal_code)\n",
    "            \"business_country\",  # ISO 3166-1 alpha-2 country code of this business.\n",
    "            \"business_latitude\",  # Latitude of the business.\n",
    "            \"business_longitude\",  # Longitude of the business.\n",
    "            \"business_categories\",  # List of categories the business belongs to.\n",
    "            \"business_parent_categories\",  # List of parent categories the business belongs to.\n",
    "        ]\n",
    "    )\n",
    "    reviews = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"business_alias\",  # Unique Yelp alias of the business.\n",
    "            \"review_text\",  # Text excerpt of this review.\n",
    "            \"review_rating\",  # Rating of this review.\n",
    "        ]\n",
    "    )\n",
    "    photos = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"business_alias\",  # Unique Yelp alias of the business.\n",
    "            \"photo_url\",  # URL of the photo.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Yelp's GraphQL endpoint\n",
    "    url = \"https://api.yelp.com/v3/graphql\"\n",
    "    # Request headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {YELP_API_KEY}\",\n",
    "        \"Content-Type\": \"application/graphql\",\n",
    "    }\n",
    "    count = 200  # Yelp's GraphQL API returns a maximum of 240 total results\n",
    "    limit = 50  # Yelp's GraphQL API returns a maximum of 50 results per request\n",
    "\n",
    "    for location in locations:\n",
    "        for offset in range(0, count, limit):\n",
    "            # Build the GraphQL query\n",
    "            query = f'{{\\n\\\n",
    "        search(categories: \"{ category }\", location: \"{ location }\", offset: { offset }, limit:  { limit }) {{\\n\\\n",
    "            business {{\\n\\\n",
    "                alias\\n\\\n",
    "                review_count\\n\\\n",
    "                rating\\n\\\n",
    "                price\\n\\\n",
    "                location {{\\n\\\n",
    "                    city\\n\\\n",
    "                    state\\n\\\n",
    "                    postal_code\\n\\\n",
    "                    country\\n\\\n",
    "                }}\\n\\\n",
    "                coordinates {{\\n\\\n",
    "                    latitude\\n\\\n",
    "                    longitude\\n\\\n",
    "                }}\\n\\\n",
    "                categories {{\\n\\\n",
    "                    alias\\n\\\n",
    "                    parent_categories {{\\n\\\n",
    "                        alias\\n\\\n",
    "                    }}\\n\\\n",
    "                }}\\n\\\n",
    "                photos\\n\\\n",
    "                reviews {{\\n\\\n",
    "                    text\\n\\\n",
    "                    rating\\n\\\n",
    "                }}\\n\\\n",
    "            }}\\n\\\n",
    "        }}\\n\\\n",
    "    }}'\n",
    "            # Send the query to the Yelp API\n",
    "            response = requests.post(url, headers=headers, data=query)\n",
    "            # Parse the response\n",
    "            if not response.status_code == 200:\n",
    "                raise Exception(\n",
    "                    f\"Yelp API request failed with status code { response.status_code }. Response text: { response.text }\"\n",
    "                )\n",
    "\n",
    "            # Parse the response\n",
    "            data = response.json()\n",
    "\n",
    "            if \"errors\" in data:\n",
    "                raise Exception(\n",
    "                    f\"Yelp API request failed with errors: { data['errors'] }\"\n",
    "                )\n",
    "\n",
    "            for business in (\n",
    "                data.get(\"data\", {}).get(\"search\", {}).get(\"business\", [])\n",
    "            ):\n",
    "                # Add the business data to the dataframe\n",
    "                businesses = businesses.append(\n",
    "                    {\n",
    "                        \"business_alias\": business.get(\"alias\"),\n",
    "                        \"business_review_count\": business.get(\"review_count\"),\n",
    "                        \"business_rating\": business.get(\"rating\"),\n",
    "                        \"business_price\": len(  # count the number of characters ($, €, ...)\n",
    "                            business.get(\"price\")\n",
    "                        )\n",
    "                        if business.get(\"price\") is not None\n",
    "                        else 0,\n",
    "                        \"business_city\": business.get(\"location\", {}).get(\n",
    "                            \"city\"\n",
    "                        ),\n",
    "                        \"business_state\": business.get(\"location\", {}).get(\n",
    "                            \"state\"\n",
    "                        ),\n",
    "                        \"business_postal_code\": business.get(\n",
    "                            \"location\", {}\n",
    "                        ).get(\"postal_code\"),\n",
    "                        \"business_country\": business.get(\"location\", {}).get(\n",
    "                            \"country\"\n",
    "                        ),\n",
    "                        \"business_latitude\": business.get(\n",
    "                            \"coordinates\", {}\n",
    "                        ).get(\"latitude\"),\n",
    "                        \"business_longitude\": business.get(\n",
    "                            \"coordinates\", {}\n",
    "                        ).get(\"longitude\"),\n",
    "                        \"business_categories\": json.dumps(\n",
    "                            list(\n",
    "                                set(  # keep unique values\n",
    "                                    [\n",
    "                                        cat.get(\"alias\")\n",
    "                                        for cat in business.get(\n",
    "                                            \"categories\", []\n",
    "                                        )\n",
    "                                    ]\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                        \"business_parent_categories\": json.dumps(\n",
    "                            list(\n",
    "                                set(  # keep unique values\n",
    "                                    [\n",
    "                                        parent_cat.get(\"alias\")\n",
    "                                        for cat in business.get(\n",
    "                                            \"categories\", []\n",
    "                                        )\n",
    "                                        for parent_cat in cat.get(\n",
    "                                            \"parent_categories\", []\n",
    "                                        )\n",
    "                                    ]\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                    },\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "                for photo in business.get(\"photos\", []) or []:\n",
    "                    # Add the photo data to the dataframe\n",
    "                    photos = photos.append(\n",
    "                        {\n",
    "                            \"business_alias\": business.get(\"alias\"),\n",
    "                            \"photo_url\": photo,\n",
    "                        },\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "\n",
    "                for review in business.get(\"reviews\", []) or []:\n",
    "                    # Add the review data to the dataframe\n",
    "                    reviews = reviews.append(\n",
    "                        {\n",
    "                            \"business_alias\": business.get(\"alias\"),\n",
    "                            \"review_text\": review.get(\"text\"),\n",
    "                            \"review_rating\": review.get(\"rating\"),\n",
    "                        },\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "\n",
    "    # Return the dataframes\n",
    "    return businesses, reviews, photos\n",
    "\n",
    "\n",
    "def download_photos(\n",
    "    photos: pd.DataFrame,\n",
    "    target_path: str,\n",
    ") -> None:\n",
    "    # Check if content path exists\n",
    "    if not os.path.exists(target_path):\n",
    "        logging.info(f\"Creating {target_path}\")\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    for photo in photos.itertuples(index=False):\n",
    "        file_name = f\"{ photo.business_alias }_{ md5(photo.photo_url.encode('utf-8')).hexdigest() }.jpg\"\n",
    "        file_path = os.path.join(target_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            request = requests.get(photo.photo_url)\n",
    "            if not 200 == request.status_code:\n",
    "                logging.warning(\n",
    "                    f\"Photo URL : { photo.photo_url }\\nYelp API request failed with status code: { request.status_code }.\\nResponse text: { request.text }\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            photo_data = request.content\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(photo_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/\"\n",
    "businesses_csv_path = os.path.join(data_path, \"businesses.csv\")\n",
    "reviews_csv_path = os.path.join(data_path, \"reviews.csv\")\n",
    "photos_csv_path = os.path.join(data_path, \"photos.csv\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    logging.info(f\"Creating {data_path}\")\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "if (\n",
    "    os.path.exists(businesses_csv_path)\n",
    "    and os.path.exists(reviews_csv_path)\n",
    "    and os.path.exists(photos_csv_path)\n",
    "):\n",
    "    logging.info(f\"Data already downloaded\")\n",
    "    businesses_df = pd.read_csv(businesses_csv_path)\n",
    "    reviews_df = pd.read_csv(reviews_csv_path)\n",
    "    photos_df = pd.read_csv(photos_csv_path)\n",
    "else:\n",
    "    logging.info(\"Saving data\")\n",
    "    businesses_df, reviews_df, photos_df = get_yelp_data()\n",
    "\n",
    "    businesses_df.to_csv(businesses_csv_path, index=False)\n",
    "    reviews_df.to_csv(reviews_csv_path, index=False)\n",
    "    photos_df.to_csv(photos_csv_path, index=False)\n",
    "\n",
    "\n",
    "# Fix dtypes\n",
    "businesses_df[\"business_alias\"] = businesses_df[\"business_alias\"].astype(str)\n",
    "businesses_df[\"business_review_count\"] = businesses_df[\n",
    "    \"business_review_count\"\n",
    "].astype(int)\n",
    "businesses_df[\"business_rating\"] = businesses_df[\"business_rating\"].astype(\n",
    "    float\n",
    ")\n",
    "businesses_df[\"business_price\"] = businesses_df[\"business_price\"].astype(int)\n",
    "businesses_df[\"business_city\"] = businesses_df[\"business_city\"].astype(str)\n",
    "businesses_df[\"business_state\"] = businesses_df[\"business_state\"].astype(str)\n",
    "businesses_df[\"business_postal_code\"] = businesses_df[\n",
    "    \"business_postal_code\"\n",
    "].astype(str)\n",
    "businesses_df[\"business_country\"] = businesses_df[\"business_country\"].astype(\n",
    "    str\n",
    ")\n",
    "businesses_df[\"business_latitude\"] = businesses_df[\"business_latitude\"].astype(\n",
    "    float\n",
    ")\n",
    "businesses_df[\"business_longitude\"] = businesses_df[\n",
    "    \"business_longitude\"\n",
    "].astype(float)\n",
    "businesses_df[\"business_categories\"] = businesses_df[\n",
    "    \"business_categories\"\n",
    "].astype(str)\n",
    "businesses_df[\"business_parent_categories\"] = businesses_df[\n",
    "    \"business_parent_categories\"\n",
    "].astype(str)\n",
    "\n",
    "reviews_df[\"business_alias\"] = reviews_df[\"business_alias\"].astype(str)\n",
    "reviews_df[\"review_text\"] = reviews_df[\"review_text\"].astype(str)\n",
    "reviews_df[\"review_rating\"] = reviews_df[\"review_rating\"].astype(float)\n",
    "\n",
    "photos_df[\"business_alias\"] = photos_df[\"business_alias\"].astype(str)\n",
    "photos_df[\"photo_url\"] = photos_df[\"photo_url\"].astype(str)\n",
    "\n",
    "\n",
    "# Reduce memory usage\n",
    "businesses_df = data_helpers.reduce_dataframe_memory_usage(businesses_df)\n",
    "reviews_df = data_helpers.reduce_dataframe_memory_usage(reviews_df)\n",
    "photos_df = data_helpers.reduce_dataframe_memory_usage(photos_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_photos(photos_df, target_path=\"../data/raw/photos/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list_variables(\n",
    "    df: pd.DataFrame,\n",
    "    columns: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode list variables.\n",
    "    See : https://cmpoi.medium.com/a-quick-tutorial-to-encode-list-variables-125ba4040325\n",
    "\n",
    "    - for each list variable\n",
    "        - decode JSON values to list\n",
    "        - make a dataframe of one-hot encoded values\n",
    "        - append to original dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to encode\n",
    "        columns (list[str]): list of columns to encode\n",
    "\n",
    "    Raises:\n",
    "        Exception: columns values should be (JSON encoded) lists of strings\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:  dataframe with encoded columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if not isinstance(df[col][0], list):\n",
    "            df[col] = df[col].replace(\n",
    "                \"[]\", \"null\"\n",
    "            )  # replace empty list with null\n",
    "            df[col] = df[col].apply(json.loads)  # convert string to list\n",
    "\n",
    "        if not isinstance(df[col][0], list):\n",
    "            raise Exception(f\"{col} is not a list\")\n",
    "\n",
    "        categories_df = (\n",
    "            pd.get_dummies(\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        x\n",
    "                        if x is not None\n",
    "                        else [\"__EMPTY__\"]  # replace None with empty list\n",
    "                        for x in df[col].tolist()\n",
    "                    ]\n",
    "                ).stack(),\n",
    "            )\n",
    "            .groupby(level=0)\n",
    "            .sum()\n",
    "        ).drop(\n",
    "            columns=\"__EMPTY__\", errors=\"ignore\"\n",
    "        )  # remove empty list\n",
    "\n",
    "        df = pd.concat([df, categories_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = one_hot_encode_list_variables(\n",
    "    businesses_df, [\"business_categories\", \"business_parent_categories\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(reviews_df, x=\"review_rating\", marginal=\"box\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = reviews_df[\"review_text\"]\n",
    "y = reviews_df[\"review_rating\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Sentiment analysis : binary classification\n",
    "y_bi = [0 if x < 5 else 1 for x in y]\n",
    "y_train_bi = [0 if x < 5 else 1 for x in y_train]\n",
    "y_test_bi = [0 if x < 5 else 1 for x in y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "tokenizers = {\n",
    "    \"None\": None,\n",
    "    \"stop_words\": lambda text: [\n",
    "        token.lower()\n",
    "        for token in word_tokenize(text)\n",
    "        if token.isalpha() and token.lower() not in stopwords\n",
    "    ],\n",
    "    \"PorterStemmer\": lambda text: [\n",
    "        PorterStemmer().stem(token).lower()\n",
    "        for token in word_tokenize(text)\n",
    "        if token.isalpha() and token.lower() not in stopwords\n",
    "    ],\n",
    "    \"WordNetLemmatizer\": lambda text: [\n",
    "        WordNetLemmatizer().lemmatize(token, pos_tagger(pos)).lower()\n",
    "        for token, pos in pos_tag(word_tokenize(text))\n",
    "        if token.isalpha() and token.lower() not in stopwords\n",
    "    ],\n",
    "    \"SpaCy\": lambda text: [\n",
    "        token.lemma_.lower()\n",
    "        for token in nlp(text)\n",
    "        if token.is_alpha and not token.is_stop\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "vectorizers = {\n",
    "    \"CountVectorizer\": CountVectorizer(),\n",
    "    \"TfidfVectorizer\": TfidfVectorizer(),\n",
    "    \"CountVectorizer + strip_accents + lowercase\": CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "    ),\n",
    "    \"TfidfVectorizer + strip_accents + lowercase\": TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "    ),\n",
    "    \"CountVectorizer + strip_accents + lowercase + stop_words\": CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "    ),\n",
    "    \"TfidfVectorizer + strip_accents + lowercase + stop_words\": TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "    ),\n",
    "    \"CountVectorizer + strip_accents + lowercase + stop_words + {min,max}_df\": CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "    ),\n",
    "    \"TfidfVectorizer + strip_accents + lowercase + stop_words + {min,max}_df\": TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "    ),\n",
    "    \"CountVectorizer + strip_accents + lowercase + stop_words + {min,max}_df + ngrams\": CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "        ngram_range=(1, 3),\n",
    "    ),\n",
    "    \"TfidfVectorizer + strip_accents + lowercase + stop_words + {min,max}_df + ngrams\": TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "        ngram_range=(1, 3),\n",
    "    ),\n",
    "}\n",
    "\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    vectorizers[f\"CountVectorizer + strip_accents + lowercase + {{min,max}}_df + ngrams + {tokenizer_name}\"] = CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "        ngram_range=(1, 3),\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    vectorizers[f\"TfidfVectorizer + strip_accents + lowercase + {{min,max}}_df + ngrams + {tokenizer_name}\"] = TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        max_df=0.9,\n",
    "        min_df=0.01,\n",
    "        ngram_range=(1, 3),\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    BayesianRidge,\n",
    "    PassiveAggressiveRegressor,\n",
    "    SGDRegressor,\n",
    "    Ridge,\n",
    "    RidgeCV,\n",
    "    Lars,\n",
    "    LarsCV,\n",
    "    Lasso,\n",
    "    LassoCV,\n",
    "    ElasticNet,\n",
    "    ElasticNetCV,\n",
    "    LassoLars,\n",
    "    LassoLarsCV,\n",
    "    OrthogonalMatchingPursuit,\n",
    "    OrthogonalMatchingPursuitCV,\n",
    "    BayesianRidge,\n",
    "    ARDRegression,\n",
    "    HuberRegressor,\n",
    "    TheilSenRegressor,\n",
    "    PassiveAggressiveRegressor,\n",
    "    SGDRegressor,\n",
    ")\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# Regression\n",
    "if not os.path.exists(\"../results/regressors_grid_search_results.csv\"):\n",
    "    pipe_reg = Pipeline(\n",
    "        [\n",
    "            (\"vec\", CountVectorizer()),\n",
    "            (\"reg\", DummyRegressor()),\n",
    "        ]\n",
    "    )\n",
    "    grid_reg = GridSearchCV(\n",
    "        pipe_reg,\n",
    "        param_grid=dict(\n",
    "            vec=[\n",
    "                CountVectorizer(),\n",
    "                TfidfVectorizer(),\n",
    "            ],\n",
    "            reg=[\n",
    "                DummyRegressor(),\n",
    "                ElasticNetCV(),\n",
    "                # TransformedTargetRegressor(\n",
    "                #     regressor=ElasticNetCV(),\n",
    "                #     transformer=QuantileTransformer(),\n",
    "                # )\n",
    "                # LinearRegression(),\n",
    "                # RidgeCV(),\n",
    "                # LarsCV(),\n",
    "                # LassoCV(),\n",
    "                # LassoLars(),\n",
    "                # LassoLarsCV(),\n",
    "                # OrthogonalMatchingPursuit(),\n",
    "                # OrthogonalMatchingPursuitCV(),\n",
    "                # BayesianRidge(),\n",
    "                # ARDRegression(),\n",
    "                # HuberRegressor(),\n",
    "                # TheilSenRegressor(),\n",
    "                # PassiveAggressiveRegressor(),\n",
    "                # SGDRegressor(),\n",
    "                # KernelRidge(),\n",
    "                # SVR(),\n",
    "                # MLPRegressor(),\n",
    "                # KNeighborsRegressor(),\n",
    "                # DecisionTreeRegressor(),\n",
    "                # RandomForestRegressor(),\n",
    "                # GradientBoostingRegressor(),\n",
    "                # LGBMRegressor(),\n",
    "            ],\n",
    "            vec__strip_accents=[None, \"unicode\"],\n",
    "            vec__max_df=[1.0, 0.99],\n",
    "            vec__min_df=[1, 0.01],\n",
    "            vec__ngram_range=[(1, 1), (1, 2)],\n",
    "            vec__tokenizer=tokenizers,\n",
    "        ),\n",
    "    ).fit(X, y)\n",
    "\n",
    "    print(grid_reg.best_estimator_)\n",
    "    print(grid_reg.best_params_)\n",
    "    print(grid_reg.best_score_)\n",
    "\n",
    "    with open(\"../results/regressors_grid_search_results.csv\", \"w\") as f:\n",
    "        pd.DataFrame(grid_reg.cv_results_).sort_values(\n",
    "            by=\"rank_test_score\",\n",
    "            ascending=True,\n",
    "        ).to_csv(\"../results/regressors_grid_search_results.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    results_reg_df = pd.read_csv(\n",
    "        \"../results/regressors_grid_search_results.csv\"\n",
    "    )\n",
    "    print(results_reg_df[[\"param_reg\", \"param_vec\", \"mean_fit_time\", \"mean_test_score\", \"rank_test_score\"]].sort_values(by=\"rank_test_score\", ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegressionCV,\n",
    "    RidgeClassifierCV,\n",
    "    SGDClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Classification\n",
    "if not os.path.exists(\"../results/classifiers_grid_search_results.csv\"):\n",
    "    pipe_cls = Pipeline(\n",
    "        [\n",
    "            (\"vec\", CountVectorizer()),\n",
    "            (\"cls\", DummyClassifier()),\n",
    "        ]\n",
    "    )\n",
    "    grid_cls = GridSearchCV(\n",
    "        pipe_cls,\n",
    "        param_grid=dict(\n",
    "            vec=[\n",
    "                CountVectorizer(),\n",
    "                TfidfVectorizer(),\n",
    "            ],\n",
    "            cls=[\n",
    "                DummyClassifier(),\n",
    "                RidgeClassifierCV(),\n",
    "                # LogisticRegressionCV(),\n",
    "                # SGDClassifier(),\n",
    "                # SVC(),\n",
    "                # KNeighborsClassifier(),\n",
    "                # MLPClassifier(),\n",
    "                # DecisionTreeClassifier(),\n",
    "                # RandomForestClassifier(),\n",
    "                # GradientBoostingClassifier(),\n",
    "                # LGBMClassifier(),\n",
    "            ],\n",
    "            vec__strip_accents=[None, \"unicode\"],\n",
    "            vec__max_df=[1.0, 0.99],\n",
    "            vec__min_df=[1, 0.01],\n",
    "            vec__ngram_range=[(1, 1), (1, 2)],\n",
    "            vec__tokenizer=tokenizers,\n",
    "        ),\n",
    "    ).fit(X, y_bi)\n",
    "\n",
    "    print(grid_cls.best_estimator_)\n",
    "    print(grid_cls.best_params_)\n",
    "    print(grid_cls.best_score_)\n",
    "\n",
    "    with open(\"../results/classifiers_grid_search_results.csv\", \"w\") as f:\n",
    "        pd.DataFrame(grid_cls.cv_results_).sort_values(\n",
    "            by=\"rank_test_score\",\n",
    "            ascending=True,\n",
    "        ).to_csv(\"../results/classifiers_grid_search_results.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    results_cls_df = pd.read_csv(\n",
    "        \"../results/classifiers_grid_search_results.csv\"\n",
    "    )\n",
    "    print(\n",
    "        results_cls_df[\n",
    "            [\n",
    "                \"param_cls\",\n",
    "                \"param_vec\",\n",
    "                \"mean_fit_time\",\n",
    "                \"mean_test_score\",\n",
    "                \"rank_test_score\",\n",
    "            ]\n",
    "        ].sort_values(by=\"rank_test_score\", ascending=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV  # Regression\n",
    "from sklearn.linear_model import RidgeClassifierCV  # Classification\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    # Regression metrics\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    # Classification metrics\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    ")\n",
    "\n",
    "\n",
    "if not os.path.exists(\n",
    "    \"../results/regression_vectorisers_results.csv\"\n",
    ") or not os.path.exists(\"../results/classification_vectorisers_results.csv\"):\n",
    "    results_reg = []\n",
    "    results_cls = []\n",
    "    for vectorizer_name, vectorizer in vectorizers.items():\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "        words_count = pd.Series(\n",
    "            X_train_vec.sum(axis=0).tolist()[0],\n",
    "            index=vectorizer.get_feature_names(),\n",
    "        )\n",
    "\n",
    "        top_20_count = words_count.sort_values(ascending=False).head(20)\n",
    "\n",
    "        fig = px.bar(\n",
    "            top_20_count,\n",
    "            x=top_20_count.index,\n",
    "            y=top_20_count.values,\n",
    "            labels={\"x\": \"Word\", \"y\": \"Count\"},\n",
    "            title=f\"{vectorizer_name} : Top 20 frequent words in reviews (vocabulary = {len(words_count)} words)\",\n",
    "            color=top_20_count.values,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Regression model\n",
    "        print(\"Regression\")\n",
    "\n",
    "        reg = ElasticNetCV(random_state=42, n_jobs=-1).fit(X_train_vec, y_train)\n",
    "        coefs_reg = pd.Series(reg.coef_, index=vectorizer.get_feature_names())\n",
    "\n",
    "        top_20_coefs_reg = (\n",
    "            coefs_reg.nlargest(10).append(coefs_reg.nsmallest(10)).sort_values()\n",
    "        )\n",
    "\n",
    "        fig = px.bar(\n",
    "            top_20_coefs_reg,\n",
    "            x=top_20_coefs_reg.index,\n",
    "            y=top_20_coefs_reg.values,\n",
    "            labels={\"x\": \"Word\", \"y\": \"Count\"},\n",
    "            title=f\"{vectorizer_name} : Top 20 important words in reviews (vocabulary = {len(words_count)} words)\",\n",
    "            color=top_20_coefs_reg.values,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        y_pred_reg = reg.predict(X_test_vec)\n",
    "\n",
    "        fig = px.box(\n",
    "            x=y_test,\n",
    "            y=y_pred_reg,\n",
    "            labels={\"x\": \"Actual\", \"y\": \"Predicted\"},\n",
    "            title=f\"{vectorizer_name} : Actual vs Predicted / R² = {round(r2_score(y_test, y_pred_reg), 3)} / MAE = {round(median_absolute_error(y_test, y_pred_reg), 3)}\",\n",
    "            color=y_test,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        results_reg.append(\n",
    "            {\n",
    "                \"vectorizer\": vectorizer_name,\n",
    "                \"vocabulary_size\": len(words_count),\n",
    "                \"r2_score\": r2_score(y_test, y_pred_reg),\n",
    "                \"median_absolute_error\": median_absolute_error(\n",
    "                    y_test, y_pred_reg\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(f\"{vectorizer_name}\")\n",
    "        print(\n",
    "            f\"vocabulary = {len(words_count)} words / R² = {round(r2_score(y_test, y_pred_reg), 3)} / MAE = {round(median_absolute_error(y_test, y_pred_reg), 3)}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Classification model\n",
    "        print(\"Classification\")\n",
    "\n",
    "        cls = RidgeClassifierCV().fit(X_train_vec, y_train_bi)\n",
    "        coefs_cls = pd.Series(\n",
    "            cls.coef_[0],\n",
    "            index=vectorizer.get_feature_names(),\n",
    "        )\n",
    "\n",
    "        top_20_coefs_cls = (\n",
    "            coefs_cls.nlargest(10).append(coefs_cls.nsmallest(10)).sort_values()\n",
    "        )\n",
    "\n",
    "        fig = px.bar(\n",
    "            top_20_coefs_cls,\n",
    "            x=top_20_coefs_cls.index,\n",
    "            y=top_20_coefs_cls.values,\n",
    "            labels={\"x\": \"Word\", \"y\": \"Count\"},\n",
    "            title=f\"{vectorizer_name} : Top 20 important words in reviews (vocabulary = {len(words_count)} words)\",\n",
    "            color=top_20_coefs_cls.values,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        y_pred_cls = cls.predict(X_test_vec)\n",
    "\n",
    "        plot_confusion_matrix(\n",
    "            estimator=cls,\n",
    "            X=X_test_vec,\n",
    "            y_true=y_test_bi,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        plot_roc_curve(\n",
    "            estimator=cls,\n",
    "            X=X_test_vec,\n",
    "            y=y_test_bi,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        plot_precision_recall_curve(\n",
    "            estimator=cls,\n",
    "            X=X_test_vec,\n",
    "            y=y_test_bi,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        results_cls.append(\n",
    "            {\n",
    "                \"vectorizer\": vectorizer_name,\n",
    "                \"vocabulary_size\": len(words_count),\n",
    "                \"accuracy_score\": accuracy_score(y_test_bi, y_pred_cls),\n",
    "                \"precision_score\": precision_score(y_test_bi, y_pred_cls),\n",
    "                \"recall_score\": recall_score(y_test_bi, y_pred_cls),\n",
    "                \"f1_score\": f1_score(y_test_bi, y_pred_cls),\n",
    "                \"roc_auc_score\": roc_auc_score(y_test_bi, y_pred_cls),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(f\"{vectorizer_name}\")\n",
    "        print(\n",
    "            f\"vocabulary = {len(words_count)} words / accuracy_score = {round(accuracy_score(y_test_bi, y_pred_cls), 3)} / precision_score = {round(precision_score(y_test_bi, y_pred_cls), 3)} / recall_score = {round(recall_score(y_test_bi, y_pred_cls), 3)} / f1_score = {round(f1_score(y_test_bi, y_pred_cls), 3)} / roc_auc_score = {round(roc_auc_score(y_test_bi, y_pred_cls), 3)}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\n",
    "        pd.DataFrame(results_reg).sort_values(\n",
    "            by=[\"r2_score\", \"median_absolute_error\"],\n",
    "            ascending=[False, True],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(\"../results/regression_vectorisers_results.csv\", \"w\") as f:\n",
    "        f.write(\n",
    "            pd.DataFrame(results_reg)\n",
    "            .sort_values(\n",
    "                by=[\"r2_score\", \"median_absolute_error\"],\n",
    "                ascending=[False, True],\n",
    "            )\n",
    "            .to_csv(index=False)\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        pd.DataFrame(results_cls).sort_values(\n",
    "            by=[\"roc_auc_score\", \"f1_score\"],\n",
    "            ascending=[False, False],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(\"../results/classification_vectorisers_results.csv\", \"w\") as f:\n",
    "        f.write(\n",
    "            pd.DataFrame(results_cls)\n",
    "            .sort_values(\n",
    "                by=[\"roc_auc_score\", \"f1_score\"],\n",
    "                ascending=[False, False],\n",
    "            )\n",
    "            .to_csv(index=False)\n",
    "        )\n",
    "\n",
    "else:\n",
    "    results_reg_df = pd.read_csv(\n",
    "        \"../results/regression_vectorisers_results.csv\"\n",
    "    )\n",
    "    print(\n",
    "        results_reg_df.sort_values(\n",
    "            by=[\"r2_score\", \"median_absolute_error\"],\n",
    "            ascending=[False, True],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results_cls_df = pd.read_csv(\n",
    "        \"../results/classification_vectorisers_results.csv\"\n",
    "    )\n",
    "    print(\n",
    "        results_cls_df.sort_values(\n",
    "            by=[\"roc_auc_score\", \"f1_score\"],\n",
    "            ascending=[False, False],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d52f4b81845109e6247863a95529093aa8c69a7aaa12f4306b437504e0244674"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
